\begin{abstract}
Transformers have become the dominant architecture for representation learning across modalities, achieving strong performance in natural language processing and computer vision. This has motivated recent efforts to extend transformers to 3D shape understanding. While these models show promising results, they face two fundamental challenges. First, transformers are notoriously data-hungry, yet even the largest 3D shape datasets remain orders of magnitude smaller than those in text or vision. Second, although transformers perform well on simple benchmarks, they struggle with tasks that require reasoning about complex geometries and topologies. In particular, it is unclear whether pretrained 3D transformer models capture the structural, non-semantic properties of shapes, beyond surface-level geometric or categorical information. In this work, we systematically investigate this question. We demonstrate that current 3D transformers are intrinsically limited in their ability to capture such structural information, and we introduce a new procedure (tbd) that addresses this limitation and improves their capacity to represent challenging 3D shape properties.
\end{abstract}